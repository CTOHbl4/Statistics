{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db137813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from sem.generate_series import create_sde_process\n",
    "from sem.sem.EM import NormalMixtureEM\n",
    "from sem.sem.windows import Windows, calculate_acf\n",
    "\n",
    "class ForecastingMixin:\n",
    "    \"\"\"\n",
    "    Mixin class providing multi-step forecasting methods.\n",
    "    Models should inherit from this mixin and nn.Module.\n",
    "    \"\"\"\n",
    "    \n",
    "    def stateless_forward_multistep(self, windows: torch.Tensor, n_steps: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Autoregressive multi-step forecasting for stateless models.\n",
    "        \n",
    "        Args:\n",
    "            windows: [batch_size, window_size] - input windows\n",
    "            n_steps: number of steps to forecast\n",
    "            \n",
    "        Returns:\n",
    "            predictions: [batch_size, n_steps]\n",
    "        \"\"\"\n",
    "        forecasts = []\n",
    "        for _ in range(n_steps):\n",
    "            forecast = self.forward(windows)\n",
    "            forecasts.append(forecast)\n",
    "\n",
    "            windows = torch.cat([\n",
    "                windows[:, 1:],\n",
    "                forecast.unsqueeze(-1)\n",
    "            ], dim=1)\n",
    "        results = torch.stack(forecasts, dim=1)\n",
    "        return results\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate_forecast(self, idx: int, \n",
    "                         time_series: torch.Tensor,\n",
    "                         n_test_steps: int,\n",
    "                         window_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate forecast starting at given index.\n",
    "        \n",
    "        Args:\n",
    "            idx: starting index (negative values count from end)\n",
    "            time_series: full time series data\n",
    "            n_test_steps: number of steps to forecast\n",
    "            window_size: size of input window\n",
    "            train_series: alias for time_series (for backward compatibility)\n",
    "            \n",
    "        Returns:\n",
    "            predictions: forecast values\n",
    "        \"\"\"\n",
    "        if idx < 0:\n",
    "            idx = time_series.shape[-1] + idx\n",
    "        idx += 1\n",
    "\n",
    "        start_idx = idx - window_size\n",
    "        ts = time_series[..., start_idx: idx]\n",
    "        assert ts.shape[-1] == window_size, 'not enough data'\n",
    "\n",
    "        if not isinstance(ts, torch.Tensor):\n",
    "            ts = torch.tensor(ts, dtype=torch.float32)\n",
    "        if ts.dim() == 1:\n",
    "            ts = ts.unsqueeze(0)\n",
    "\n",
    "        device = next(self.parameters()).device\n",
    "        window = ts.to(device)\n",
    "\n",
    "        preds = self.forward_multistep(window, n_test_steps)\n",
    "        if ts.shape[0] == 1:\n",
    "            preds = preds.squeeze(0)\n",
    "            \n",
    "        return preds\n",
    "\n",
    "    def forward_multistep(self, windows: torch.Tensor, n_steps: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Default multi-step forecasting implementation.\n",
    "        Can be overridden by subclasses for stateful models.\n",
    "        \n",
    "        Args:\n",
    "            windows: [batch_size, window_size]\n",
    "            n_steps: number of steps to forecast\n",
    "            \n",
    "        Returns:\n",
    "            predictions: [batch_size, n_steps]\n",
    "        \"\"\"\n",
    "        return self.stateless_forward_multistep(windows, n_steps)\n",
    "\n",
    "\n",
    "class EMForecaster(ForecastingMixin, nn.Module):\n",
    "    def __init__(self, \n",
    "                 window_size: int,\n",
    "                 n_components: int = 3,\n",
    "                 hidden_dim: int = 64,\n",
    "                 n_em_iters: int = 5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dwindow_size = window_size - 1\n",
    "        self.window_size = window_size\n",
    "        self.n_components = n_components\n",
    "        negative_slope = 0.01\n",
    "\n",
    "        self.em_layer = NormalMixtureEM(\n",
    "            series_length=self.dwindow_size,\n",
    "            n_components=n_components,\n",
    "            n_sem_iters=n_em_iters\n",
    "        )\n",
    "\n",
    "        fusion_input_dim = self.dwindow_size + n_components * 5\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(fusion_input_dim, hidden_dim, bias=False),\n",
    "            nn.LeakyReLU(negative_slope),\n",
    "            nn.Linear(hidden_dim, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(module.weight, negative_slope)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, X_window):\n",
    "\n",
    "        dX1 = X_window[..., 1:] - X_window[..., :-1]\n",
    "        g_ik, p_k, a_k, b_k = self.em_layer(dX1)\n",
    "\n",
    "        attention_logits = -b_k\n",
    "        attention_weights = nn.functional.softmax(attention_logits, dim=1)\n",
    "\n",
    "        fusion_input = torch.cat([\n",
    "            dX1,\n",
    "            a_k,\n",
    "            b_k,\n",
    "            p_k,\n",
    "            attention_weights * a_k,\n",
    "            p_k * a_k\n",
    "        ], dim=1)\n",
    "        forecast = X_window[..., -1] + self.fusion(fusion_input).squeeze(-1)\n",
    "        \n",
    "        return forecast\n",
    "\n",
    "\n",
    "class Baseline(ForecastingMixin, nn.Module):\n",
    "    def __init__(self, window_size, hidden_dim=80):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.dwindow_size = window_size - 1\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.dwindow_size, hidden_dim // 2, bias=False),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(hidden_dim // 2, 1, bias=False)\n",
    "        )\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, a=0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dx = x[:, 1:] - x[:, :-1]\n",
    "        \n",
    "        delta = self.fusion(dx).squeeze(-1)\n",
    "        return x[:, -1] + delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import json\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, time_series: np.ndarray, window_size, n_train_steps=1):\n",
    "        super().__init__()\n",
    "        self.ts = time_series\n",
    "        self.n_train_steps = n_train_steps\n",
    "        self.window_length = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ts) - self.window_length - self.n_train_steps\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx_last = idx + self.window_length\n",
    "        return self.ts[idx:idx_last], self.ts[idx_last: idx_last + self.n_train_steps]\n",
    "\n",
    "\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        mape = torch.abs(predictions - targets) / (torch.abs(targets) + self.eps)\n",
    "        return mape.mean()\n",
    "\n",
    "\n",
    "class WeightedLoss(nn.Module):\n",
    "    def __init__(self, mape=0.1, mse=0.9, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.mape = MAPELoss(eps)\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.mape_weight = mape\n",
    "        self.mse_weight = mse\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        return self.mape_weight * self.mape(predictions, targets) + self.mse_weight * self.mse(predictions, targets)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, setup, time_series, window_size, len_train, log_path, batch_size=64, \n",
    "                 log_freq_batch=50, n_tests=100, n_test_steps=50, n_train_steps=1, eps=1e-8, device='cuda'):\n",
    "        self.train_series = time_series[:len_train]\n",
    "        self.eval_series = time_series[len_train:]\n",
    "        dataset = TimeSeriesDataset(self.train_series, window_size=window_size, n_train_steps=n_train_steps)\n",
    "        self.dataloader = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.n_epochs = setup['n_epochs']\n",
    "        self.n_tests = n_tests\n",
    "        self.n_test_steps = n_test_steps\n",
    "        self.model = setup['model_class'](**setup['model_args']).to(device)\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), setup['lr'])\n",
    "        self.criterion = WeightedLoss()\n",
    "        self.scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "            self.optimizer, 1.0, 1e-8, self.n_epochs\n",
    "        )\n",
    "\n",
    "        self.loss_threshold = 0.01\n",
    "        self.eps = eps\n",
    "        self.device = device\n",
    "        self.log_freq_batch = log_freq_batch\n",
    "        self.log_path = log_path\n",
    "        \n",
    "        self.n_train_steps = n_train_steps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        preds = self.model.generate_forecast(\n",
    "            idx=-1,\n",
    "            time_series=self.train_series,\n",
    "            n_test_steps=self.n_test_steps,\n",
    "            window_size=self.window_size\n",
    "        ).cpu().numpy()\n",
    "        target = self.eval_series[:self.n_test_steps]\n",
    "        return preds, target\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def evaluate_multistep(self, test_ids):\n",
    "        preds = np.empty((self.n_tests + 1, self.n_test_steps))\n",
    "        targets = np.empty((self.n_tests + 1, self.n_test_steps))\n",
    "        for test in range(self.n_tests):\n",
    "            idx = test_ids[test]\n",
    "            target_idx = idx + 1\n",
    "            pred = self.model.generate_forecast(\n",
    "                idx=idx,\n",
    "                time_series=self.train_series,\n",
    "                n_test_steps=self.n_test_steps,\n",
    "                window_size=self.window_size\n",
    "            ).cpu()\n",
    "            target = self.train_series[target_idx:target_idx + self.n_test_steps]\n",
    "            preds[test, :] = pred.numpy()\n",
    "            targets[test, :] = target\n",
    "        return preds, targets\n",
    "\n",
    "    def run(self, test_ids):\n",
    "        epoch_train_time = 0\n",
    "        epoch_val_time = 0\n",
    "        start_full = time()\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            self.model.train()\n",
    "            start_train = time()\n",
    "            avg_threshold = self.train_epoch(epoch)\n",
    "            epoch_train_time += time() - start_train\n",
    "            self.model.eval()\n",
    "\n",
    "            start_test = time()\n",
    "            preds, targets = self.evaluate_multistep(test_ids)\n",
    "            pred, target = self.validate()\n",
    "            epoch_val_time += time() - start_test\n",
    "            preds[-1, :] = pred\n",
    "            targets[-1, :] = target\n",
    "            tests_results = np.stack([preds, targets], axis=-1)  # shape: [n_tests+1, n_steps, 2]\n",
    "            \n",
    "            np.save(self.log_path / f'epoch_{epoch}', tests_results)\n",
    "            \n",
    "            if avg_threshold < self.loss_threshold:\n",
    "                break\n",
    "        \n",
    "        time_full = time() - start_full\n",
    "        mean_train_time = epoch_train_time / epoch\n",
    "        mean_test_time = epoch_val_time / epoch / (self.n_test_steps + 1) / (self.n_tests)\n",
    "\n",
    "        json.dump(\n",
    "                {\n",
    "                    'full_time': float(time_full),\n",
    "                    'epoch_train_time': float(mean_train_time),\n",
    "                    'epoch_test_time_one_step': float(mean_test_time),\n",
    "                    'last_loss': float(avg_threshold)\n",
    "                },\n",
    "                open(self.log_path / 'times.json', 'w'),\n",
    "                indent=2\n",
    "            )\n",
    "        print()\n",
    "        print(\"Training completed!\")\n",
    "        return self.model\n",
    "    \n",
    "    def train_epoch(self, epoch_idx=0):\n",
    "        print(\"Epoch:\", epoch_idx)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch_idx, (windows, y) in enumerate(self.dataloader):\n",
    "            windows = windows.to(self.device).float()\n",
    "            y = y.to(self.device).float()\n",
    "            if self.n_train_steps == 1:\n",
    "                y = y.squeeze(-1)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            if self.n_train_steps == 1:\n",
    "                results = self.model.forward(windows)\n",
    "                if isinstance(results, tuple):\n",
    "                    results = results[0]\n",
    "            else:\n",
    "                results = self.model.forward_multistep(windows, self.n_train_steps)\n",
    "\n",
    "            loss = self.criterion(results, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch_idx % self.log_freq_batch == 0:\n",
    "                basic_metrics = self.compute_basic_metrics(results, y)\n",
    "                print(\"Batch IDX:\", batch_idx)\n",
    "                print(basic_metrics)\n",
    "                print()\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"WARNING: NaN loss at batch {batch_idx}\")\n",
    "                break\n",
    "        self.scheduler.step()\n",
    "\n",
    "        avg_loss = total_loss / len(self.dataloader)\n",
    "        print(f\"Epoch {epoch_idx} - Average Loss: {avg_loss:.6f}\")\n",
    "        return avg_loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_basic_metrics(self, forecast, y_true):\n",
    "        mae = torch.abs(forecast - y_true).mean().item()\n",
    "        mse = torch.mean((forecast - y_true) ** 2).item()\n",
    "        mape = torch.abs((forecast - y_true) / (y_true.abs() + self.eps)).mean().item() * 100\n",
    "        return {'MAE': mae, 'MSE': mse, 'MAPE': mape}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ba699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_setups(window_size):\n",
    "    base_configs = []\n",
    "    \n",
    "    # MLP Model\n",
    "    base_configs.append({\n",
    "        'model_class': Baseline,\n",
    "        'model_args': {'window_size': window_size, 'hidden_dim': 64},\n",
    "        'name': f'MLP',\n",
    "        'lr': 1e-3,\n",
    "        'n_epochs': 150\n",
    "    })\n",
    "    \n",
    "    # EMForecaster Model\n",
    "    em_configs = [\n",
    "        {'n_components': 2},\n",
    "        {'n_components': 3},\n",
    "    ]\n",
    "    \n",
    "    for config in em_configs:\n",
    "        base_configs.append({\n",
    "            'model_class': EMForecaster,\n",
    "            'model_args': {\n",
    "                'window_size': window_size,\n",
    "                'n_components': config['n_components'],\n",
    "                'hidden_dim': 64,\n",
    "                'n_em_iters': 5\n",
    "            },\n",
    "            'name': f'EM_n{config[\"n_components\"]}',\n",
    "            'lr': 1e-3,\n",
    "            'n_epochs': 150\n",
    "        })\n",
    "    \n",
    "    return base_configs\n",
    "\n",
    "def create_experiment_folder(base_path=\"experiments\", model_name=None, series_idx=None):\n",
    "    \"\"\"\n",
    "    Create folder structure: experiments/<model_name>/<series_idx>/\n",
    "    \"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = \"default\"\n",
    "    if series_idx is None:\n",
    "        series_idx = \"0\"\n",
    "    \n",
    "    exp_path = Path(base_path) / model_name / str(series_idx)\n",
    "    exp_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return exp_path\n",
    "\n",
    "def generate_test_ids(window_size, train_series, n_tests, n_test_steps):\n",
    "    res = []\n",
    "    for _ in range(n_tests):\n",
    "        res.append(random.randint(window_size, len(train_series) - 1 - n_test_steps))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162e5b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tests = 10\n",
    "series_length = 5000\n",
    "train_length = 4500\n",
    "alpha = 0.75\n",
    "N_init = 5\n",
    "N_add = 5\n",
    "n_epoch_tests = 40\n",
    "n_test_steps = 200\n",
    "\n",
    "for test in range(n_tests):\n",
    "    print('_' * 50)\n",
    "    print(test)\n",
    "    print('_' * 50)\n",
    "    set_seed(10 * test)\n",
    "    series = create_sde_process(series_length)['X']\n",
    "    train_series = series[:train_length]\n",
    "    *_, window_size = Windows()(train_series[1:] - train_series[:-1], alpha, N_init, N_add)\n",
    "    window_size += 1\n",
    "    setups = get_setups(window_size)\n",
    "    test_ids = generate_test_ids(window_size, train_series, n_epoch_tests, n_test_steps)\n",
    "    for setup in setups:\n",
    "        set_seed(10 * test)\n",
    "        print('_' * 50)\n",
    "        print(setup['name'])\n",
    "        print('_' * 50)\n",
    "\n",
    "        log_path = create_experiment_folder(model_name=setup['name'], series_idx=test)\n",
    "        if len(os.listdir(log_path)):\n",
    "            print(log_path)\n",
    "            continue\n",
    "        trainer = Trainer(setup, series, window_size, train_length, log_path,\n",
    "                          batch_size=64, log_freq_batch=70, n_tests=n_epoch_tests, n_test_steps=n_test_steps, device='cuda')\n",
    "        model = trainer.run(test_ids)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
